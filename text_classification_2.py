# -*- coding: utf-8 -*-
"""Text-Classification-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DP87Y6Jx7yAcnCxqP5k6jZ7IVbBU5I7s

## **1: Data Exploration**
"""

import pandas as pd

# Load dataset
df = pd.read_csv("text_class - text_class.csv")

# Display first 5 rows
print(df.head())

# Total number of rows and unique labels
print(f"Total rows: {len(df)}")
print(f"Unique labels: {df['label'].nunique()} - {df['label'].unique()}")

# Check for missing values
print(df.isnull().sum())

"""**Explanation:**
We first load the dataset and inspect the top 5 rows. We then check how many entries exist and how many unique classification labels are present. A check for missing data is a necessary step before preprocessing.

**2: Preprocessing Text Data**
"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download stopwords and punkt if not already
nltk.download('punkt')
nltk.download('stopwords')
# Download punkt_tab as suggested by the error message
nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))

def preprocess(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^a-z\s]', '', text)  # Remove punctuation and special chars
    tokens = word_tokenize(text)  # Tokenize
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# Apply preprocessing
df['processed_text'] = df['text'].apply(preprocess)

# Show processed version of first 5 rows
print(df[['text', 'processed_text']].head())

"""**Explanation:**
We convert all text to lowercase, remove punctuation and special characters using regex, tokenize it into words, and remove common stopwords using NLTK.

**3: Train a Classifier**
"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Vectorize the processed text
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['processed_text'])
y = df['label']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Predict and calculate accuracy
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.2f}")

"""**Explanation:**
We convert the processed text to numerical features using TF-IDF, then train a Logistic Regression classifier on the training set and evaluate accuracy on the test set.

**4: Evaluate the Model**
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

"""**Explanation:**
The confusion matrix helps us understand how many samples were correctly and incorrectly classified. Itâ€™s especially useful for analyzing which labels are being confused with others.
"""